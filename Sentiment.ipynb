{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/elibol/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elibol/anaconda3/envs/py27/lib/python2.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # For sentiment analysis\n",
    "import cPickle as pickle # For loaded dataset from pickle file\n",
    "import tqdm # Progress bar\n",
    "from collections import Counter # Handy addon\n",
    "from pprint import pprint # Useful to print JSON objects\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset of articles with introductions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b92b23279593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mproject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'$project'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pubtime'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"title\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"source\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"url\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"introductions\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ntopic'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "# This is internal, to generate the dataset, feel free to remove it in your file\n",
    "ntopics = {15664: \"Brexit\", 14723: \"ISIS War\"}\n",
    "match = {'$match': {'ntopic': {'$in': ntopics.keys()}}}\n",
    "project = {'$project': {'_id': 0, 'pubtime': 1, \"title\": 1, \"source\": 1, \"url\": 1, \"introductions\": 1, 'ntopic': 1}}\n",
    "\n",
    "articles = list(db.article.aggregate([match, project]))\n",
    "\n",
    "\n",
    "for article in articles:\n",
    "    article['news_topic'] = ntopics[article['ntopic']]\n",
    "    del article['ntopic']\n",
    "\n",
    "with open(\"news_sentiment.pickle\", \"w\") as f:\n",
    "    pickle.dump(articles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57767 articles were loaded\n",
      "Example article:\n",
      "{u'introductions': [{u'person': u'Bashar al-Assad',\n",
      "                     u'text': u'President',\n",
      "                     u'wdid': u'Q44329'},\n",
      "                    {u'person': u'Emile Hokayem',\n",
      "                     u'text': u'in Foreign Policy'},\n",
      "                    {u'person': u'Ahrar al Sham',\n",
      "                     u'text': u'the most important groups',\n",
      "                     u'wdid': u'Q860943'},\n",
      "                    {u'person': u'Vladimir Putin',\n",
      "                     u'text': u'Russian President',\n",
      "                     u'wdid': u'Q7747'},\n",
      "                    {u'person': u'Barack Obama',\n",
      "                     u'text': u'U.S. President',\n",
      "                     u'wdid': u'Q76'},\n",
      "                    {u'person': u'Osama Abu Zeid',\n",
      "                     u'text': u'a senior adviser to the moderate Free Syrian Army'},\n",
      "                    {u'person': u'Op-Ed',\n",
      "                     u'text': u'for The Washington Post',\n",
      "                     u'wdid': u'Q2602337'},\n",
      "                    {u'person': u'Nicholas Burns',\n",
      "                     u'text': u'two former senior officials',\n",
      "                     u'wdid': u'Q7025139'},\n",
      "                    {u'person': u'Natalie Nougayr\\xe8de',\n",
      "                     u'text': u'Guardian columnist',\n",
      "                     u'wdid': u'Q6430048'},\n",
      "                    {u'person': u'Natalie Nougayr\\xe8de',\n",
      "                     u'text': u'',\n",
      "                     u'wdid': u'Q6430048'}],\n",
      " 'news_topic': 'ISIS War',\n",
      " u'pubtime': datetime.datetime(2016, 2, 8, 15, 18, 6),\n",
      " u'source': u'cnn.com',\n",
      " u'title': u'Aleppo siege marks upheaval on Syrian battlefield',\n",
      " u'url': u'http://www.cnn.com/2016/02/07/middleeast/syria-aleppo-siege/index.html?eref=rss_world'}\n"
     ]
    }
   ],
   "source": [
    "# This loads the file that you want, might take several seconds (up to a minute)\n",
    "\n",
    "with open(\"news_sentiment.pickle\", \"r\") as f:\n",
    "    articles = pickle.load(f)\n",
    "print len(articles), \"articles were loaded\"\n",
    "print \"Example article:\"\n",
    "pprint(articles[1040])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39206  articles from ISIS War and  18561 articles from Brexit were loaded\n"
     ]
    }
   ],
   "source": [
    "# separate articles from the two stories\n",
    "ISIS_articles = []\n",
    "Brexit_articles = []\n",
    "for a in articles:\n",
    "    if a[\"news_topic\"] == 'ISIS War':\n",
    "        ISIS_articles.append(a)\n",
    "    else:\n",
    "        Brexit_articles.append(a)\n",
    "        \n",
    "print len(ISIS_articles), \" articles from ISIS War and \", len(Brexit_articles), \"articles from Brexit were loaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only articles from one story, you can change this\n",
    "articles = ISIS_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract introductions, and obtain their sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dcf635af0a4dd981bf876d904d3846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=214880), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer() \n",
    "\n",
    "total_introductions = []\n",
    "for a in articles:\n",
    "    for intro in a.get('introductions', []): # get intros from each article\n",
    "        intro['source'] = a['source']\n",
    "        total_introductions.append(intro) # total intros across\n",
    "\n",
    "# call the sentiment analysis (VADER) func to output a sentiment val from the text \n",
    "for intro in tqdm.tqdm_notebook(total_introductions):\n",
    "    intro['sentiment'] = analyzer.polarity_scores(intro['text'])['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Entity mentionned: Asaad Hanna\n",
      "a spokesman for the Free Syrian Army\n",
      "Sentiment: 0.5106\n",
      "---------------\n",
      "Entity mentionned: Gulen\n",
      "who has denied any involvement in the failed putsch\n",
      "Sentiment: -0.7351\n",
      "---------------\n",
      "Entity mentionned: Al-Naim\n",
      "where the jihadists once displayed the severed heads of their enemies\n",
      "Sentiment: -0.6908\n",
      "---------------\n",
      "Entity mentionned: Moammar Gadhafi\n",
      "Libyan strongman\n",
      "Sentiment: 0.1779\n",
      "---------------\n",
      "Entity mentionned: Hama\n",
      "a center of resistance where activists said dozens had been killed in new attacks\n",
      "Sentiment: -0.8126\n",
      "---------------\n",
      "Entity mentionned: Fethullah Gulen\n",
      "the US - based preacher who is accused of masterminding the failed July coup aimed at ousting President Recep Tayyip Erdogan\n",
      "Sentiment: -0.6705\n",
      "---------------\n",
      "Entity mentionned: Ake Sellstrom\n",
      "a former U.N. weapons inspector in Iraq\n",
      "Sentiment: -0.4404\n",
      "---------------\n",
      "Entity mentionned: Davutoglu\n",
      "a more mild - mannered academic and former diplomat who lacks Erdogan 's natural appeal to crowds\n",
      "Sentiment: 0.3612\n",
      "---------------\n",
      "Entity mentionned: Ibrahim al-Masri\n",
      "a 37-year - old Hariri supporter\n",
      "Sentiment: 0.2732\n",
      "---------------\n",
      "Entity mentionned: Ahmad Chalabi\n",
      "the secular Shi'ite politician whose false assertions about weapons of mass destruction encouraged the Bush administration to invade Iraq\n",
      "Sentiment: -0.6249\n",
      "---------------\n",
      "Entity mentionned: Fateh al-Sham\n",
      "of which Ahrar al - Sham has been the most prominent\n",
      "Sentiment: 0.3804\n",
      "---------------\n",
      "Entity mentionned: Ashura\n",
      "the religious ritual that commemorates the death of the Prophet Mohammad 's grandson Imam Hussein in 680\n",
      "Sentiment: -0.5994\n",
      "---------------\n",
      "Entity mentionned: Samir Kantar\n",
      "now - dead\n",
      "Sentiment: -0.6486\n"
     ]
    }
   ],
   "source": [
    "# Example some sentiment for some of the introductions\n",
    "\n",
    "subsample = np.random.choice(total_introductions, 100)\n",
    "for intro in subsample:\n",
    "    if intro['sentiment'] != 0: #print a few <entity,apposition,sentiment> values where sentiment !=0\n",
    "        print \"---------------\"\n",
    "        print \"Entity mentionned:\", intro['person']\n",
    "        print intro['text']\n",
    "        print \"Sentiment:\", intro['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a 2-dimensional object containing sentiment per entity, per source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_source_sent = {}\n",
    "\n",
    "for intro in total_introductions:\n",
    "    p = intro['person']\n",
    "    s = intro['source']\n",
    "    if p not in ent_source_sent:\n",
    "        ent_source_sent[p] = {} #allocate space to fille ent_source_sent\n",
    "    if s not in ent_source_sent[p]:\n",
    "        ent_source_sent[p][s] = [] #allocate space to fille ent_source_sent\n",
    "    ent_source_sent[p][s].append(intro['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'nytimes.com': [0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, -0.5574, 0.0, 0.0, 0.0, 0.0], u'allafrica.com': [-0.5994], u'bloomberg.com': [-0.5994, 0.0, 0.0, -0.2023, 0.0, -0.4404, -0.1531, -0.1531, 0.0, 0.0], u'bbc.co.uk': [0.0516, 0.0, -0.1531, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, -0.3182, -0.5994, -0.5994, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0], u'theguardian.com': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.7096, 0.0, -0.1531, 0.0], u'telegraph.co.uk': [0.4019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3182, 0.4404, -0.296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3818, -0.1531, 0.0, -0.1531, 0.0, 0.0, 0.0, -0.3182, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3612, 0.2023, -0.1531, 0.0, 0.0, -0.1531, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], u'independent.co.uk': [-0.1531, 0.0, -0.0258, -0.1531, -0.6597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1779, 0.0, 0.0, -0.2023, 0.0, -0.2023, 0.0, -0.2023, 0.0, -0.2023, 0.0, 0.0, 0.0, 0.0, -0.2023, 0.0, 0.0, 0.0, -0.2023, 0.0, -0.2023, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2023, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5994, 0.0, 0.0, 0.0, -0.2023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2023, 0.0, -0.2023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2023, 0.0, 0.0, -0.5994, 0.0, 0.0, 0.0, 0.0, -0.2023, 0.0, -0.2023, -0.2023, 0.0, -0.2023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], u'washingtonpost.com': [0.0, 0.0, 0.0, 0.0, -0.5994, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, -0.5859], u'foxnews.com': [0.0, 0.5709, -0.5994, 0.0, -0.5994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5994, 0.0, -0.5994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, -0.1531, 0.0, -0.1531, 0.0, -0.5994, 0.0, 0.0, -0.1531, -0.6249, 0.0, -0.6249, 0.0, -0.6249, -0.5574, -0.8402, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.2732, 0.0, -0.4939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0], u'aa.com.tr': [0.0, -0.5994, -0.4019, 0.0, 0.0, 0.0, -0.5994, 0.0, -0.5994, 0.0, 0.0, 0.4588, -0.5994, 0.0, 0.0, 0.0, -0.7184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], u'rt.com': [0.5574, 0.5574, 0.0, 0.0, 0.0, -0.5994, 0.0, -0.5994, -0.5994, 0.0, 0.0, 0.0, 0.0, -0.8316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6908], u'france24.com': [-0.3182, 0.0, -0.6705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2023, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, -0.5023, 0.0, 0.0, 0.0, -0.1531, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.743, 0.0, 0.0, 0.0, 0.0, -0.5994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.4767, -0.5994, -0.9136, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5106, -0.6808, -0.3818, 0.0, 0.0, 0.0, 0.0, 0.3182, 0.4404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8176, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5994, 0.0, 0.0, -0.6249, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5023, 0.0, 0.0, 0.5023, 0.0258, 0.0258, 0.0, -0.1531, -0.1531, 0.0, 0.0, -0.1531, 0.0, 0.0, -0.1531, -0.1531, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1779, -0.1531, 0.0, -0.4019, 0.0, 0.0, -0.5994, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, -0.1531, -0.1531, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, -0.3818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.128, 0.0, 0.0], u'latimes.com': [0.0, 0.0], u'cnn.com': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0516, 0.0, 0.0, 0.0, -0.5994, -0.3818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.7096, -0.7096, 0.0, 0.0, 0.0, -0.3612, 0.0, 0.0, 0.0, 0.0, 0.0, -0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1779], u'reuters.com': [-0.1531, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, -0.5994, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3612, -0.3612, 0.0, 0.0, 0.0, 0.0, -0.5106, 0.0, 0.0, 0.0, -0.3818, 0.0, 0.3182, 0.4404, 0.0, 0.3182, 0.4404, 0.0, -0.4404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3182, -0.3182, -0.5994, 0.0, -0.4588, 0.0, 0.0, -0.6808, 0.0, -0.4767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4404, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.6124, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2732, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.128, 0.128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1779, 0.0, -0.1779, -0.1531, -0.5994, -0.1531, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2732, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, -0.1531, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.128, 0.128, 0.0, 0.0, 0.0], u'chinadaily.com.cn': [0.0, 0.0], u'middleeasteye.net': [-0.1531, 0.0, 0.0, 0.0, 0.0, -0.1531, -0.4404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3612, 0.0, 0.0, -0.1531, 0.0, 0.0, -0.1531, 0.0, 0.0, -0.1531, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], u'aljazeera.com': [-0.3182, 0.0, 0.0, -0.1531, 0.0, 0.0, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, -0.1531, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1531, 0.0, -0.34, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0516, -0.0516, -0.1531, 0.0, 0.0, -0.4404, -0.4404], u'ap.org': [0.0, -0.1531, 0.0, 0.0, 0.0, -0.5994, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5709, 0.0, 0.0, 0.0, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "# An example of how one entity (a city) is described by different sources\n",
    "\n",
    "print ent_source_sent['Aleppo'] # sentiments across sources for the single entity 'Aleppo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will keep a total of 7852  /  25128  entities in our dataset\n",
      "We have  22 sources:  [u'telegraph.co.uk', u'foxnews.com', u'ap.org', u'businessinsider.in', u'independent.co.uk', u'reuters.com', u'wikinews.org', u'cnn.com', u'techcrunch.com', u'aa.com.tr', u'allafrica.com', u'nytimes.com', u'bloomberg.com', u'bbc.co.uk', u'latimes.com', u'rt.com', u'france24.com', u'chinadaily.com.cn', u'theguardian.com', u'washingtonpost.com', u'middleeasteye.net', u'aljazeera.com']\n"
     ]
    }
   ],
   "source": [
    "# We get rid of entities that don't contain enough data\n",
    "\n",
    "entities_kept = []\n",
    "\n",
    "for entity in ent_source_sent.keys():\n",
    "    sentiments = ent_source_sent[entity] # collect sentiments across all sources for certain entity\n",
    "    total_size = sum([len(sentiments[source]) for source in sentiments.keys()])\n",
    "    if total_size >= 3: # only keep entities that 3 or more sources mention\n",
    "        entities_kept.append(entity)\n",
    "print \"We will keep a total of\", len(entities_kept), \" / \", len(ent_source_sent.keys()) ,\" entities in our dataset\"\n",
    "\n",
    "sources = set([])\n",
    "for entity in entities_kept:\n",
    "    sources|= set(ent_source_sent[entity].keys())\n",
    "sources = list(sources)\n",
    "\n",
    "print \"We have \", len(sources), \"sources: \", sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We create the array we will use in our sparse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We allocated some sentiment in this matrix, the repartition is: Counter({0: 19061, 1: 3650, -1: 2670})\n"
     ]
    }
   ],
   "source": [
    "# this converts sentiments for same actor for same source into aggregate sentiment (which is discrete and is 1,0,or-1)\n",
    "# Parameters: changing these affects the results you get\n",
    "Pos_neg_ratio = 2.0\n",
    "overall_ratio = 0.15\n",
    "pos_threshold = 0.15\n",
    "neg_threshold = -0.15\n",
    "\n",
    "N = len(entities_kept)\n",
    "M = len(sources)\n",
    "A = np.zeros((N, M))\n",
    "\n",
    "sentiment_counts = Counter()\n",
    "\n",
    "source2j = {source: j for j, source in enumerate(sources)}\n",
    "\n",
    "for i, entity in enumerate(entities_kept):\n",
    "    for source in ent_source_sent[entity].keys():\n",
    "        sent_array = np.array(ent_source_sent[entity][source])\n",
    "        N_pos = float(len(np.where(sent_array > pos_threshold)[0])) # count sentiments that are positive enough\n",
    "        N_neg = float(len(np.where(sent_array < neg_threshold)[0]))\n",
    "        T = float(len(sent_array))\n",
    "        aggregate_sentiment = 0\n",
    "        if N_pos > Pos_neg_ratio*N_neg and N_pos > overall_ratio*T:\n",
    "            aggregate_sentiment = 1 \n",
    "        elif N_neg > Pos_neg_ratio*N_pos and N_neg > overall_ratio*T:\n",
    "            aggregate_sentiment = -1\n",
    "        j = source2j[source]\n",
    "        \n",
    "        A[i,j] = aggregate_sentiment\n",
    "        \n",
    "        sentiment_counts[aggregate_sentiment] += 1 #keeps track of #1,0,-1s assigned\n",
    "\n",
    "print \"We allocated some sentiment in this matrix, the repartition is:\", sentiment_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model source similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write code that uses this matrix (entities, sources) to compute\n",
    "# source similarity visible in bias of the way they describe entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "businessinsider.in foxnews.com\n",
      "reuters.com ap.org\n",
      "wikinews.org ap.org\n",
      "wikinews.org businessinsider.in\n",
      "wikinews.org independent.co.uk\n",
      "wikinews.org reuters.com\n",
      "cnn.com ap.org\n",
      "techcrunch.com foxnews.com\n",
      "techcrunch.com ap.org\n",
      "techcrunch.com independent.co.uk\n",
      "techcrunch.com reuters.com\n",
      "aa.com.tr techcrunch.com\n",
      "allafrica.com businessinsider.in\n",
      "allafrica.com reuters.com\n",
      "nytimes.com ap.org\n",
      "nytimes.com allafrica.com\n",
      "bbc.co.uk ap.org\n",
      "bbc.co.uk techcrunch.com\n",
      "latimes.com telegraph.co.uk\n",
      "latimes.com reuters.com\n",
      "latimes.com techcrunch.com\n",
      "latimes.com nytimes.com\n",
      "latimes.com bloomberg.com\n",
      "rt.com ap.org\n",
      "rt.com businessinsider.in\n",
      "rt.com latimes.com\n",
      "france24.com ap.org\n",
      "france24.com wikinews.org\n",
      "france24.com techcrunch.com\n",
      "france24.com allafrica.com\n",
      "france24.com latimes.com\n",
      "chinadaily.com.cn cnn.com\n",
      "chinadaily.com.cn nytimes.com\n",
      "theguardian.com techcrunch.com\n",
      "theguardian.com allafrica.com\n",
      "theguardian.com france24.com\n",
      "washingtonpost.com chinadaily.com.cn\n",
      "middleeasteye.net foxnews.com\n",
      "middleeasteye.net techcrunch.com\n",
      "middleeasteye.net allafrica.com\n",
      "middleeasteye.net nytimes.com\n",
      "middleeasteye.net washingtonpost.com\n",
      "aljazeera.com techcrunch.com\n"
     ]
    }
   ],
   "source": [
    "# reference on sklearn's graph lasso: http://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphLasso.html\n",
    "from sklearn.covariance import GraphLasso # our Algo code should replace this and input/output the same thing\n",
    "graph_lasso = GraphLasso(alpha=0.00001) # alpha =  regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance.\n",
    "graph_lasso.fit(A) # A is the aggregated sentiment matrix, an arrray of (n_samples, n_features)\n",
    "np.mean(graph_lasso.get_precision() > 0) #calculates avg of the precision matrix elements that are >0\n",
    "\n",
    "# print pairs of sources for which precision matrix has pos val\n",
    "# when precision matrix is pos, source pairs are likely to have same sentiment\n",
    "# when precision matrix is neg, source pairs are likely to have opposit sentiment\n",
    "for (i, j) in zip(*np.where(graph_lasso.get_precision() > 0)):\n",
    "    if i > j: #since precision matrix is symmetric, only need to print upper half\n",
    "        print sources[i], sources[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "778194441efa4fe0ac005b5453b5c790": {
     "views": [
      {
       "cell_index": 6
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
